# Awesome-LLM-Eval

[English](README_EN.md) | [中文](README_CN.md)

Awesome-LLM-Eval: a curated list of tools, datasets/benchmark, demos, learderboard, papers, docs and models, mainly for Evaluation on Large Language Models (like ChatGPT, LLaMA, GLM, Baichuan, etc).

## Table of Contents

- [Tools](#Tools)
- [Datasets / Benchmark](#Datasets-or-Benchmark)
- [Demos](#Demos)
- [Leaderborad](#Leaderborad)
- [Papers](#papers)
- [LLM-List](#LLM-List)
  - [Pre-trained LLM](#Pre-trained-LLM)
  - [Instruction finetuned LLM](#Instruction-finetuned-LLM)
  - [Aligned LLM](#Aligned-LLM)
  - [Open LLM](#Open-LLM)
  - [Popular LLM](#Popular-LLM)
- [LLMOps](#LLMOps)
- [Frameworks for Training](#Frameworks-for-Training)
- [Courses](#Courses)
- [Others](#Others)
- [Other-Awesome-Lists](#Other-Awesome-Lists)
- [Licenses](#Licenses)
- [引用](#引用)

![](docs/survey-gif-test.gif)
![](docs/image_llm_palm.gif)


## Tools

| 名称 | 机构 | 网址 | 简介 |
| :--: | :--: | :--: | :--: |
| EVAL | OPENAI | [EVAL](https://github.com/openai/evals) | EVAL是OpenAI开发的一个用于评估大型语言模型（LLM）的工具，可以测试模型在不同任务和数据集上的性能和泛化能力. |
| lm-evaluation-harness | EleutherAI | [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) | EVAL是OpenAI开发的一个用于评估大型语言模型（LLM）的工具，可以测试模型在不同任务和数据集上的性能和泛化能力. |
| Large language model evaluation and workflow framework from Phase AI | wgryc | [phasellm](https://github.com/wgryc/phasellm) | phasellm是Phase AI提供的一个用于评估和管理LLM的框架，可以帮助用户选择合适的模型、数据集和指标，以及可视化和分析结果. |
| Evaluation benchmark for LLM | FreedomIntelligence | [LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) | LLMZoo是FreedomIntelligence开发的一个用于评估LLM的基准，包含了多个领域和任务的数据集和指标，以及一些预训练的模型和结果. |
| Holistic Evaluation of Language Models (HELM) | Stanford | [HELM](https://github.com/stanford-crfm/helm) | HELM是Stanford研究团队提出的一个用于全面评估LLM的方法，考虑了模型的语言能力、知识、推理、公平性和安全性等多个方面. |
| A lightweight evaluation tool for question-answering | Langchain | [auto-evaluator](https://github.com/rlancemartin/auto-evaluator) | auto-evaluator是Langchain开发的一个用于评估问答系统的轻量级工具，可以自动生成问题和答案，并计算模型的准确率、召回率和F1分数等指标. |
| PandaLM | WeOpenML | [PandaLM](https://github.com/WeOpenML/PandaLM) | ReProducible and Automated Language Model Assessment | PandaLM是WeOpenML开发的一个用于自动化和可复现地评估LLM的工具，可以根据用户的需求和偏好，选择合适的数据集、指标和模型，并生成报告和图表. |
| FlagEval | Tsinghua University | [FlagEval](https://github.com/FlagOpen/FlagEval) | FlagEval是清华大学开发的一个用于评估LLM的平台，可以提供多种任务和数据集，以及在线测试、排行榜和分析等功能 |
| AlpacaEval | tatsu-lab | [alpaca_eval](https://github.com/tatsu-lab/alpaca_eval) | AlpacaEval是tatsu-lab开发的一个用于评估LLM的工具，可以使用多种语言、领域和任务进行测试，并提供可解释性、鲁棒性和可信度等指标. |
|Prompt flow | Microsoft | [promptflow](github.com/microsoft/promptflow) | 一套开发工具，旨在简化基于 LLM 的AI应用的端到端开发周期，从构思、原型设计、测试、评估到生产部署和监控。它使提示工程变得更加容易，使您能构建具有产品级质量的 LLM 应用. |
| DeepEval | mr-gpt | [DeepEval](github.com/mr-gpt/deepeval) | DeepEval：提供一种 Pythonic 方式在 LLM 管线上运行离线评估，以便轻松投入生产 |


## Datasets-or-Benchmark

| 数据名称 | 机构 | 网址 | 简介 |
| :--: | :--: | :--: | :--: |
| M3Exam | DAMO | [M3Exam](https://github.com/DAMO-NLP-SG/M3Exam) | A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models. |
| KoLA | THU-KEG | [KoLA](http://103.238.162.37:31622) | Knowledge-oriented LLM Assessment benchmark (KoLA), is hosted by Knowledge Engineering Group, Tsinghua University (THU-KEG), which aims at carefully benchmarking the world knowledge of LLMs by undertaking meticulous designs considering data, ability taxonomy and evaluation metric. |
| promptbench | microsoft | [promptbench](https://github.com/microsoft/promptbench) | PromptBench is a powerful tool designed to scrutinize and analyze the interaction of large language models with various prompts. It provides a convenient infrastructure to simulate black-box adversarial prompt attacks on the models and evaluate their performances. This repository hosts the necessary codebase, datasets, and instructions to facilitate these experiments. |
| OpenCompass | Shanghai AI Lab | [OpenCompass](https://github.com/InternLM/opencompass/tree/main) | OpenCompass 是面向大模型评测的一站式平台。其主要特点如下：开源可复现：提供公平、公开、可复现的大模型评测方案；全面的能力维度：五大维度设计，提供 50+ 个数据集约 30 万题的的模型评测方案，全面评估模型能力；丰富的模型支持：已支持 20+ HuggingFace 及 API 模型；分布式高效评测：一行命令实现任务分割和分布式评测，数小时即可完成千亿模型全量评测；多样化评测范式：支持零样本、小样本及思维链评测，结合标准型或对话型提示词模板，轻松激发各种模型最大性能 |
| JioNLP-LLM评测数据集 | jionlp | [JioNLP-LLM评测数据集](https://github.com/dongrixinyu/JioNLP/wiki/LLM评测数据集) | LLM 评测数据集主要用于评测通用 LLM 的效果评价。着眼考察 LLM 模型对人类用户的帮助效果、辅助能力，可否达到一个【智能助手】的水平。题型包括：选择题来源于中国大陆国内各种专业性考试，重点在于考察模型对客观知识的覆盖面，占比 32%；主观题来源于日常总结，主要考察用户对 LLM 常用功能的效果 |
| BIG-bench | google | [BIG-bench](https://github.com/google/BIG-bench) | BIG bench由 204 项任务组成，任务主题涉及语言学、儿童发展、数学、常识推理、生物学、物理学、社会偏见、软件开发等等领域的问题 |
| BIG-Bench-Hard | Stanford NLP | [BIG-Bench-Hard](https://github.com/suzgunmirac/BIG-Bench-Hard) | A suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater |
| SuperCLUE | CLUEbenchmark | [SuperCLUE](https://github.com/CLUEbenchmark/SuperCLUE) | 中文的一个榜单，这里从基础能力、专业能力、中文特性三个角度进行准备测试集 基础能力能力包括：语义理解、对话、逻辑推理、角色模拟、代码、生成与创作等10项能力。专业能力包括：包括了中学、大学与专业考试，涵盖了从数学、物理、地理到社会科学等50多项能力。中文特性能力：针对有中文特点的任务，包括了中文成语、诗歌、文学、字形等10项多种能力 |
| Safety Eval | 清华大学 | [Safety Eval 安全大模型评测](http://115.182.62.166:18000) | 清华收集的一个评测集，涵盖了仇恨言论、偏见歧视言论、犯罪违法、隐私、伦理道德等八大类别，包括细粒度划分的40余个二级安全类别，并依托于一套系统的安全评测框架 |
| GAOKAO-Bench | OpenLMLab | [GAOKAO-Bench](https://github.com/OpenLMLab/GAOKAO-Bench) | GAOKAO-bench是一个以中国高考题目为数据集，测评大模型语言理解能力、逻辑推理能力的测评框架 |
| Gaokao | ExpressAI | [Gaokao](https://github.com/ExpressAI/AI-Gaokao) | “高考基准”旨在评估和追踪我们在达到人类智力水平方面取得的进展。它不仅可以提供对现实世界场景中实际有用的不同任务和领域的全面评估，还提供丰富的人类表现，以便大模型等可以直接与人类进行比较 |
| MMLU | paperswithcode.com | [MMLU](https://paperswithcode.com/dataset/mmlu) | 该测评数据集涵盖 STEM、人文学科、社会科学等领域的 57 个学科。难度从初级到专业高级，既考验世界知识，又考验解决问题的能力。学科范围从数学和历史等传统领域到法律和伦理等更专业的领域。主题的粒度和广度使基准成为识别模型盲点的理想选择 |
| CMMLU | MBZUAI & ShangHai JiaoTong & Microsoft | [CMMLU](https://github.com/haonan-li/CMMLU) | Measuring massive multitask language understanding in Chinese |
| MMCU | 甲骨易AI研究院 | [MMCU](https://github.com/Felixgithub2017/MMCU) | 甲骨易AI研究院提出一种衡量中文大模型处理多任务准确度的测试, 数据集的测试内容涵盖四大领域：医疗、法律、心理学和教育。题目的数量达到1万+，其中包括医疗领域2819道题，法律领域3695道题，心理学领域2001道，教育领域3331道 |
| AGIEval | 微软研究院 | [AGIEval](https://www.microsoft.com/en-us/research/project/agi-evaluation/) | 由微软研究院发起，旨在全面评估基础模型在人类认知和问题解决相关任务上的能力，包含了中国的高考、司法考试，以及美国的SAT、LSAT、GRE和GMAT等20个公开且严谨的官方入学和职业资格考试|
| C_Eval | 上交、清华以及爱丁堡大学 | [C_Eval](https://cevalbenchmark.com/index.html#home) | 上交、清华以及爱丁堡大学合作产出的一个评测集，包含52个学科来评估大模型高级知识和推理能力，其评估了包含 GPT-4、ChatGPT、Claude、LLaMA、Moss 等多个模型的性能。|
| XieZhi | Fudan Univesity | [XieZhi](https://github.com/MikeGu721/XiezhiBenchmark) | A comprehensive evaluation suite for Language Models (LMs). It consists of 249587 multi-choice questions spanning 516 diverse disciplines and four difficulty levels. 新的领域知识综合评估基准测试：Xiezhi。对于多选题，Xiezhi涵盖了516种不同学科中的220,000个独特问题，其中涵盖了13个学科。作者还提出了Xiezhi-Specialty和Xiezhi-Interdiscipline，每个都含有15k个问题。使用Xiezhi基准测试评估了47种先进的LLMs的性能|
| MT-bench | UC Berkeley, UCSD, CMU, Stanford, MBZUAI | [MT-bench](https://arxiv.org/abs/2306.05685) | A benchmark consisting of 80 high-quality multi-turn questions. MT-bench is designed to test multi-turn conversation and instruction-following ability, covering common use cases and focusing on challenging questions to differentiate models. It includes 8 common categories of user prompts to guide its construction: writing, roleplay, extraction, reasoning, math, coding, etc. | 
| GLUE Benchmark | NYU, University of Washington, DeepMind, Facebook AI Research, Allen Institute for AI, Google AI Language | [GLUE Benchmark](https://gluebenchmark.com/) | 评估模型在语法、改写、文本相似度、推理、文本蕴含、代词指代等任务上的表现 |
| OpenAI Moderation API | OpenAI | [OpenAI Moderation API](https://platform.openai.com/docs/api-reference/moderations) | 过滤有害或不安全的内容 |
| GSM8K | OpenAI | [GSM8K](https://github.com/openai/grade-school-math) | GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. GSM8K segmented these into 7.5K training problems and 1K test problems. These problems take between 2 and 8 steps to solve, and solutions primarily involve performing a sequence of elementary calculations using basic arithmetic operations (+ - / *) to reach the final answer. |
| EleutherAI LM Eval | EleutherAI | [EleutherAI LM Eval](https://github.com/EleutherAI/lm-evaluation-harness) | 评估模型在少量样本下的表现和在多种任务上的微调效果 |
| OpenAI Evals | OpenAI | [OpenAI Evals](https://github.com/openai/evals) | 评估生成文本的准确性、多样性、一致性、鲁棒性、迁移性、效率和公平性 |
| AlpacaEval | tatsu-lab | [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval) | An LLM-based automatic evaluation that is fast, cheap, and reliable. It is based on the AlpacaFarm evaluation set, which tests the ability of models to follow general user instructions. These responses are then compared to reference Davinci003 responses by the provided GPT-4 or Claude or ChatGPT based auto-annotators, which results in the win rates presented above. |
| Adversarial NLI (ANLI) | Facebook AI Research, New York University, Johns Hopkins University, University of Maryland, Allen Institute for AI | [Adversarial NLI (ANLI)](https://github.com/facebookresearch/anli) | 评估模型在对抗样本下的鲁棒性、泛化能力、推理解释能力和一致性，以及资源使用效率（内存使用、推理时间和训练时间）|
| LIT (Language Interpretability Tool) | Google |	[LIT](https://pair-code.github.io/lit/)	| 提供一个平台，可以根据用户定义的指标进行评估，分析模型的优势、弱点和潜在偏差 |
| ParlAI	| Facebook AI Research	| [ParlAI](https://github.com/facebookresearch/ParlAI)	| 评估模型在准确性、F1分数、困惑度（模型预测序列中下一个词的能力）、人类评价（相关性、流畅度和连贯性）、速度和资源利用率、鲁棒性（模型在不同条件下的表现，如噪声输入、对抗攻击或数据质量变化）、泛化能力等方面的表现 |
| CoQA	| Stanford NLP Group	| [CoQA](https://stanfordnlp.github.io/coqa/)	| 评估模型在理解文本段落并回答一系列相互关联的问题，这些问题出现在对话中的能力 |
| LAMBADA	| University of Trento and Fondazione Bruno Kessler	| [LAMBADA](https://zenodo.org/record/2630551#.ZFUKS-zML0p)	| 评估模型使用预测段落最后一个词的方式来衡量长期理解能力 |
| HellaSwag	| University of Washington and Allen Institute for AI	| [HellaSwag](https://rowanzellers.com/hellaswag/)	| 评估模型的推理能力 |
| LogiQA	| Tsinghua University and Microsoft Research Asia	| [LogiQA](https://github.com/lgw863/LogiQA-dataset)	| 评估模型的逻辑推理能力 |
| MultiNLI	| New York University, DeepMind, Facebook AI Research, Allen Institute for AI, Google AI Language | [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) | 评估模型在不同文体之间理解句子关系的能力 |
| SQUAD	| Stanford NLP Group	| [SQUAD](https://rajpurkar.github.io/SQuAD-explorer/)	| 评估模型在阅读理解任务上的表现 |
| Open LLM Leaderboard | HuggingFace | [Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) | 由HuggingFace组织的一个LLM评测榜单，目前已评估了较多主流的开源LLM模型。评估主要包括AI2 Reasoning Challenge, HellaSwag, MMLU, TruthfulQA四个数据集上的表现，主要以英文为主 |
| chinese-llm-benchmark | jeinlee1991 | [llm-benchmark](https://github.com/jeinlee1991/chinese-llm-benchmark) | 中文大模型能力评测榜单：覆盖百度文心一言、chatgpt、阿里通义千问、讯飞星火、belle / chatglm6b 等开源大模型，多维度能力评测。不仅提供能力评分排行榜，也提供所有模型的原始输出结果 |
| AlpacaEval | tatsu-lab | [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/) | 开源模型领先者 vicuna、openchat 和 wizardlm 的基于LLM的自动评估|
| Huggingface开源LLM排行榜 | huggingface | [HF开源LLM排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) | 仅评估开源模型，在 Eleuther AI 的四个评估集上排名，Falcon 夺冠，vicuna 亦夺冠|
| lmsys-arena | Berkley | [lmsys排名榜](https://lmsys.org/blog/2023-05-03-arena/) | 使用 Elo 评分机制，排名为 GPT4 > Claude > GPT3.5 > Vicuna > 其他|
| CMU开源聊天机器人评测应用 | CMU | [zeno-build](https://github.com/zeno-ml/zeno-build) | 在对话场景中进行训练可能很重要，排名为 ChatGPT > Vicuna > 其他|
| Z-Bench中文真格基金评测 | 真格基金 | [ Z-Bench](https://github.com/zhenbench/z-bench) | 国产中文模型的编程可用性相对较低，模型水平差异不大，两个版本的 ChatGLM 有明显提升|
| Chain-of-thought评估 | Yao Fu | [COT评估](https://github.com/FranxYao/chain-of-thought-hub) | 包括 GSM8k、MATH 等复杂问题排名|
| InfoQ大模型综合能力评估 | InfoQ | [InfoQ评测](https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&mid=2651170429&idx=1&sn=b98af3bd14c9f97f1aa07f0f839bb3ec&scene=21#wechat_redirect) | 面向中文，排名为 ChatGPT > 文心一言 > Claude > 星火|
| ToolBench工具调用评测 | 智源/清华 | [ToolBench](https://github.com/OpenBMB/ToolBench) | 通过与工具微调模型和 ChatGPT 进行比较，提供评测脚本|
| AgentBench推理决策评估榜单 | THUDM | [AgentBench](https://github.com/THUDM/AgentBench) | 清华联合多所高校推出，涵盖不同任务环境，如购物、家居、操作系统等场景下模型的推理决策能力|
| FlagEval | 智源/清华 | [FlagEval](https://flageval.baai.ac.cn/#/home) | 智源出品，结合主观和客观评分，提供了LLM的评分榜单|
| ChatEval | THU-NLP | [ChatEval](https://github.com/thunlp/ChatEval) |ChatEval旨在简化人类对生成的文本进行评估的过程。当给定不同的文本片段时，ChatEval中的角色(由法学硕士扮演)可以自主地讨论细微差别和差异，利用他们指定的角色，随后提供他们的判断|
| Zhujiu | Institute of Automation, CAS | [Zhujiu](http://www.zhujiu-benchmark.com) | 多维能力覆盖，涵盖了7个能力维度和51个任务；多方面的评估方法协作，综合使用3种不同但互补的评估方法；全面的中文基准测试，同时提供英文评估能力 |
| LucyEval | 甲骨文 | [LucyEval](http://lucyeval.besteasy.com/) | 中文大语言模型成熟度评测——LucyEval，能够通过对模型各方面能力的客观测试，找到模型的不足，帮助设计者和工程师更加精准地调整、训练模型，助力大模型不断迈向更智能的未来 |
| Do-Not-Answer | Libr-AI | [Do-Not-Answer](https://github.com/Libr-AI/do-not-answer)| "Do not answer" 是一个开源数据集，旨在以低成本评估LLM（大型语言模型）的安全机制。该数据集经过策划和筛选，仅包含那些负责任的语言模型不回答的提示。除了人工标注外，“Do not answer” 还实施了基于模型的评估，其中一个经过6亿次微调的类似BERT的评估器获得了与人类和GPT-4相媲美的结果 |
| Aviary | github.com/ray-project/aviary | [Aviary](github.com/ray-project/aviary) |允许在一个地方与各种大型语言模型(LLM)进行交互。可以直接比较不同模型的输出，按质量进行排名，获得成本和延迟估计等功能。特别支持在Hugging Face上托管的Transformer模型，并在许多情况下还支持DeepSpeed推理加速 (202306) |
| AgentBench | Tsinghua University | [AgentBench](https://github.com/THUDM/AgentBench) | AgentBench是一个用于评估LLM作为agent智能体的系统化基准评测工具，突出了商业LLM和开源竞争对手之间的性能差距 (202308)|
| LLMEval²-WideDeep | AlibabaResearch | [LLMEval²](https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/WideDeep) | 构建了最大、最多样化的英语评估基准LLMEval²，供LLM评估者使用，包括15个任务、8个能力和2,553个样本。实验结果表明，一个更宽的网络（涉及许多审阅者）和2层（一轮讨论）的性能最佳，将Kappa相关系数从0.28提高到0.34。我们还利用WideDeep来辅助评估中文LLM，这加速了评估时间4.6倍，节省了60%的成本 |
| FinEval | SUFE-AIFLM-Lab | [FinEval](github.com/SUFE-AIFLM-Lab/FinEval) | FinEval：包含金融、经济、会计和证书等领域高质量多项选择题的集合 |

## Demos
- [Chat Arena: anonymous models side-by-side and vote for which one is better](https://chat.lmsys.org/?arena) - 开源AI大模型“匿名”竞技场！你在这里可以成为一名裁判，给两个事先不知道名字的模型回答打分，评分后将给出他们的真实身份。目前已经“参赛”的选手包括Vicuna、Koala、OpenAssistant (oasst)、Dolly、ChatGLM、StableLM、Alpaca、LLaMA等。


## Leaderborad

### Performance from XieZhi-202306

|       Models      |                MMLU                |                            |                            |            CEval           |                            |                            |            M3KE            |    Xiezhi-Spec.-Chinese    |                            |                            |    Xiezhi-Inter.-Chinese   |                            |                            |    Xiezhi-Spec.-English    |                            |                            |    Xiezhi-Inter.-English   |                            |                            |
|:--------------------------:|:----------------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|:--------------------------:|
|                            |               0-shot               |           1-shot           |           3-shot           |           0-shot           |           1-shot           |           3-shot           |           0-shot           |           0-shot           |           1-shot           |           3-shot           |           0-shot           |           1-shot           |           3-shot           |           0-shot           |           1-shot           |           3-shot           |           0-shot           |           1-shot           |           3-shot           |
|        Random-Guess        |                0.089               |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |            0.089           |
|                            | Generation Probability For Ranking |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |
|         Bloomz-560m        |                0.111               |            0.109           |            0.119           |            0.124           |            0.117           |            0.103           |            0.126           |            0.123           |            0.127           |            0.124           |            0.130           |            0.138           |            0.140           |            0.113           |            0.116           |            0.123           |            0.124           |            0.117           |            0.160           |
|         Bloomz-1b1         |                0.131               |            0.116           |            0.128           |            0.107           |            0.115           |            0.110           |            0.082           |            0.138           |            0.108           |            0.107           |            0.117           |            0.125           |            0.123           |            0.130           |            0.119           |            0.114           |            0.144           |            0.129           |            0.145           |
|         Bloomz-1b7         |                0.107               |            0.117           |            0.164           |            0.054           |            0.058           |            0.103           |            0.102           |            0.165           |            0.151           |            0.159           |            0.152           |       **0.214**       |            0.170           |            0.133           |            0.140           |            0.144           |            0.150           |            0.149           |            0.209           |
|          Bloomz-3b         |                0.139               |            0.084           |            0.146           |            0.168           |       **0.182**       |            0.194           |            0.063           |            0.186           |            0.154           |            0.168           |            0.151           |            0.180           |            0.182           |            0.201           |            0.155           |            0.156           |            0.175           |            0.164           |            0.158           |
|         Bloomz-7b1         |                0.167               |            0.160           |            0.205           |            0.074           |            0.072           |            0.073           |            0.073           |            0.154           |            0.178           |            0.162           |            0.148           |            0.160           |            0.156           |            0.176           |            0.153           |            0.207           |            0.217           |            0.204           |            0.229           |
|        Bloomz-7b1-mt       |                0.189               |            0.196           |            0.210           |            0.077           |            0.078           |            0.158           |            0.072           |            0.163           |            0.175           |            0.154           |            0.155           |            0.195           |            0.164           |            0.180           |            0.146           |            0.219           |            0.228           |            0.171           |            0.232           |
|        Bloomz-7b1-p3       |                0.066               |            0.059           |            0.075           |            0.071           |            0.070           |            0.072           |            0.081           |            0.177           |            0.198           |            0.158           |            0.183           |            0.173           |            0.170           |            0.130           |            0.130           |            0.162           |            0.157           |            0.132           |            0.134           |
|           Bloomz           |                0.051               |            0.066           |            0.053           |            0.142           |            0.166           | **0.240** |            0.098           |            0.185           |            0.133           |            0.277           |            0.161           |            0.099           |       **0.224**       |            0.069           |            0.082           |            0.056           |            0.058           |            0.055           |            0.049           |
|          Bloomz-mt         |     **0.266**     | **0.264** | **0.248** | **0.204** |            0.164           |            0.151           |       **0.161**       | **0.253** |       **0.198**       |       **0.212**       |       **0.213**       |            0.189           |            0.184           | **0.379** | **0.396** | **0.394** | **0.383** | **0.405** | **0.398** |
|          Bloomz-p3         |                0.115               |            0.093           |            0.057           |            0.118           |            0.137           |            0.140           |            0.115           |            0.136           |            0.095           |            0.105           |            0.086           |            0.065           |            0.098           |            0.139           |            0.097           |            0.069           |            0.176           |            0.141           |            0.070           |
|          llama-7b          |                0.125               |       **0.132**       |            0.093           |            0.133           |            0.106           |            0.110           |       **0.158**       |       **0.152**       |            0.141           |            0.117           |            0.142           |            0.135           |            0.128           |            0.159           |            0.165           |            0.161           |       **0.194**       |            0.183           |            0.176           |
|          llama-13b         |           **0.166**           |            0.079           |       **0.135**       |            0.152           |       **0.181**       |       **0.169**       |            0.131           |            0.133           | **0.241** |       **0.243**       |       **0.211**       |       **0.202**       | **0.303** |            0.154           |            0.183           |       **0.215**       |            0.174           |       **0.216**       |       **0.231**       |
|          llama-30b         |                0.076               |            0.107           |            0.073           |            0.079           |            0.119           |            0.082           |            0.079           |            0.140           |            0.206           |            0.162           |            0.186           |       **0.202**       |            0.183           |            0.110           |            0.195           |            0.161           |            0.088           |            0.158           |            0.219           |
|          llama-65b         |                0.143               |            0.121           |            0.100           |       **0.154**       |            0.141           |            0.168           |            0.125           |            0.142           |            0.129           |            0.084           |            0.108           |            0.077           |            0.077           |       **0.183**       |       **0.204**       |            0.172           |            0.133           |            0.191           |            0.157           |
|       baize-7b~(lora)      |                0.129               |            0.091           |            0.079           |       **0.194**       |            0.180           |       **0.206**       | **0.231** |       **0.216**       |            0.148           |            0.123           |            0.173           |            0.158           |            0.198           |       **0.182**       |            0.190           |            0.194           |       **0.218**       |            0.188           |            0.209           |
| baize-7b-healthcare~(lora) |                0.130               |            0.121           |            0.106           |            0.178           |            0.174           |            0.178           |            0.203           |            0.178           |            0.146           |            0.123           | **0.266** |            0.107           |            0.118           |            0.175           |            0.164           |            0.173           |            0.197           |       **0.231**       |            0.198           |
|      baize-13b~(lora)      |                0.131               |            0.111           |            0.171           |            0.184           |            0.178           |            0.195           |            0.155           |            0.158           |       **0.221 **      | **0.256** |            0.208           |            0.200           |       **0.219**       |            0.176           |            0.189           |       **0.239**       |            0.187           |            0.185           |       **0.274**       |
|      baize-30b~(lora)      |           **0.193**           |       **0.216**       |       **0.207**       |            0.191           | **0.196** |            0.121           |            0.071           |            0.109           |            0.212           |            0.190           |            0.203           | **0.256** |            0.200           |            0.167           |       **0.235**       |            0.168           |            0.072           |            0.180           |            0.193           |
|         Belle-0.2M         |                0.127               |       **0.148**       |       **0.243**       |            0.053           |            0.063           |       **0.136**       |       **0.076**       |            0.172           |            0.126           |            0.153           |            0.171           |            0.165           |            0.147           |            0.206           |            0.146           |            0.148           |       **0.217**       |            0.150           |            0.173           |
|         Belle-0.6M         |                0.091               |            0.114           |            0.180           |       **0.082**       |       **0.080**       |            0.090           |            0.075           |       **0.188**       |            0.149           |       **0.198**       |       **0.188**       |       **0.188**       |            0.175           |            0.173           |       **0.172**       |       **0.183**       |            0.193           |       **0.184**       |       **0.196**       |
|          Belle-1M          |           **0.137**           |            0.126           |            0.162           |            0.066           |            0.065           |            0.072           |            0.066           |            0.170           |            0.152           |            0.147           |            0.173           |            0.176           |       **0.197**       |       **0.211**       |            0.137           |            0.149           |            0.207           |            0.151           |            0.185           |
|          Belle-2M          |                0.127               |       **0.148**       |            0.132           |            0.058           |            0.063           |       **0.136**       |            0.057           |            0.163           |       **0.166**       |            0.130           |            0.159           |            0.177           |            0.163           |            0.155           |            0.106           |            0.166           |            0.151           |            0.150           |            0.138           |
|         chatglm-6B         |           **0.099**           |       **0.109**       |       **0.112**       |       **0.084**       |            0.074           |       **0.114**       |       **0.115**       |       **0.082**       |       **0.097**       |       **0.147**       |       **0.104**       |       **0.111**       |       **0.144**       |       **0.106**       |       **0.120**       |       **0.124**       |            0.099           |       **0.079**       |       **0.097**       |
|        doctorglm-6b        |                0.093               |            0.076           |            0.065           |            0.037           |       **0.085**       |            0.051           |            0.038           |            0.062           |            0.068           |            0.044           |            0.047           |            0.056           |            0.043           |            0.069           |            0.053           |            0.043           |            0.106           |            0.059           |            0.059           |
|        moss-base-16B       |           **0.072**           |            0.050           |       **0.062**       |       **0.115**       |            0.048           |            0.052           |       **0.099**       |       **0.105**       |            0.051           |            0.059           |       **0.123**       |            0.054           |            0.058           |       **0.124**       |       **0.077**       |       **0.080**       |       **0.121**       |            0.058           |            0.063           |
|        moss-sft-16B        |                0.064               |       **0.065**       |            0.051           |            0.063           |       **0.062**       |       **0.072**       |            0.075           |            0.072           |       **0.067**       |       **0.068**       |            0.073           |       **0.081**       |       **0.066**       |            0.071           |            0.070           |            0.059           |            0.074           |       **0.084**       |       **0.075**       |
|          vicuna-7b         |                0.051               |            0.051           |            0.029           |            0.063           |            0.071           |            0.064           |            0.059           |            0.169           |       **0.171**       |            0.165           |            0.134           |       **0.201**       |       **0.213**       |            0.182           |       **0.209**       |       **0.195**       |       **0.200**       |       **0.214**       |            0.182           |
|         vicuna-13b         |                0.109               |            0.104           |            0.066           |            0.060           |       **0.131**       |       **0.131**       |            0.067           |       **0.171**       |            0.167           |       **0.166**       |            0.143           |            0.147           |            0.178           |            0.121           |            0.139           |            0.128           |            0.158           |            0.174           |       **0.191**       |
|          alpaca-7b         |           **0.135**           |       **0.170**       |       **0.202**       |       **0.137**       |            0.119           |            0.113           |       **0.142**       |            0.129           |            0.139           |            0.123           |       **0.178**       |            0.104           |            0.097           |       **0.189**       |            0.179           |            0.128           |       **0.200**       |            0.185           |            0.149           |
|         pythia-1.4b        |           **0.124**           |       **0.127**       |       **0.121**       |       **0.108**       |       **0.132**       |       **0.138**       |            0.083           |       **0.125**       |       **0.128**       |       **0.135**       |            0.111           |            0.146           |            0.135           |       **0.158**       |       **0.124**       |       **0.124**       |       **0.166**       |            0.126           |            0.118           |
|         pythia-2.8b        |                0.103               |            0.110           |            0.066           |            0.064           |            0.089           |            0.122           |            0.086           |            0.114           |            0.120           |            0.131           |            0.091           |            0.113           |            0.112           |            0.126           |            0.118           |            0.112           |            0.110           |       **0.145**       |            0.107           |
|         pythia-6.9b        |                0.115               |            0.070           |            0.084           |            0.078           |            0.073           |            0.094           |            0.073           |            0.086           |            0.094           |            0.092           |            0.097           |            0.098           |            0.085           |            0.091           |            0.088           |            0.083           |            0.099           |            0.099           |            0.096           |
|         pythia-12b         |                0.075               |            0.059           |            0.066           |            0.077           |            0.097           |            0.078           |       **0.098**       |            0.102           |            0.126           |            0.132           |       **0.125**       |       **0.147**       |       **0.159**       |            0.079           |            0.098           |            0.110           |            0.094           |            0.120           |       **0.120**       |
|        gpt-neox-20b        |                0.081               |       **0.132**       |            0.086           |            0.086           |       **0.096**       |            0.069           |            0.094           |       **0.140**       |       **0.103**       |       **0.109**       |       **0.120**       |       **0.098**       |            0.085           |            0.088           |       **0.101**       |       **0.116**       |            0.099           |       **0.113**       |       **0.156**       |
|         h2ogpt-12b         |                0.075               |            0.087           |            0.078           |            0.080           |            0.078           |       **0.094**       |            0.070           |            0.065           |            0.047           |            0.073           |            0.076           |            0.061           |       **0.091**       |            0.088           |            0.050           |            0.065           |            0.105           |            0.063           |            0.067           |
|         h2ogpt-20b         |           **0.114**           |            0.098           |       **0.110**       |       **0.094**       |            0.084           |            0.061           |       **0.096**       |            0.108           |            0.080           |            0.073           |            0.086           |            0.081           |            0.072           |       **0.108**       |            0.068           |            0.086           |       **0.109**       |            0.071           |            0.079           |
|          dolly-3b          |                0.066               |            0.060           |            0.055           |            0.079           |       **0.083**       |       **0.077**       |            0.066           |            0.100           |            0.090           |            0.083           |            0.091           |            0.093           |            0.085           |            0.079           |            0.063           |            0.077           |            0.076           |            0.074           |            0.084           |
|          dolly-7b          |           **0.095**           |       **0.068**       |            0.052           |       **0.091**       |            0.079           |            0.070           |            0.108           |       **0.108**       |            0.089           |            0.092           |       **0.111**       |            0.095           |            0.100           |       **0.096**       |            0.059           |            0.086           |       **0.123**       |            0.085           |            0.090           |
|          dolly-12b         |           **0.095**           |       **0.068**       |       **0.093**       |            0.085           |            0.071           |            0.073           |       **0.114**       |            0.098           |       **0.106**       |       **0.103**       |            0.094           |       **0.114**       |       **0.106**       |            0.086           |       **0.088**       |       **0.098**       |            0.088           |       **0.102**       |       **0.116**       |
|         stablelm-3b        |                0.070               |            0.085           |            0.071           |            0.086           |            0.082           |       **0.099**       |            0.096           |       **0.101**       |            0.087           |            0.091           |            0.083           |            0.092           |            0.067           |            0.069           |            0.089           |            0.081           |            0.066           |            0.085           |            0.088           |
|         stablelm-7b        |           **0.158**           |       **0.118**       |       **0.093**       |       **0.133**       |       **0.102**       |            0.093           |       **0.140**       |            0.085           |       **0.118**       |       **0.122**       |       **0.123**       |       **0.130**       |       **0.095**       |       **0.123**       |       **0.103**       |       **0.100**       |       **0.134**       |       **0.121**       |       **0.105**       |
|          falcon-7b         |                0.048               |            0.046           |            0.051           |            0.046           |            0.051           |            0.052           |            0.050           |            0.077           |       **0.096**       |       **0.112**       |       **0.129**       |       **0.141**       |       **0.142**       |            0.124           |            0.103           |            0.107           |       **0.198**       |       **0.200**       |       **0.205**       |
|     falcon-7b-instruct     |                0.078               |            0.095           |            0.106           |       **0.114**       |       **0.095**       |            0.079           |            0.104           |            0.075           |            0.083           |            0.087           |            0.060           |            0.133           |            0.123           |       **0.160**       |       **0.203**       |       **0.156**       |            0.141           |            0.167           |            0.152           |
|         falcon-40b         |                0.038               |            0.043           |            0.077           |            0.085           |            0.090           |       **0.129**       |            0.087           |            0.069           |            0.056           |            0.053           |            0.065           |            0.063           |            0.058           |            0.059           |            0.077           |            0.066           |            0.085           |            0.063           |            0.076           |
|     falcon-40b-instruct    |           **0.126**           |       **0.123**       |       **0.121**       |            0.070           |            0.080           |            0.068           |       **0.141**       |       **0.103**       |            0.085           |            0.079           |            0.115           |            0.082           |            0.081           |            0.118           |            0.143           |            0.124           |            0.083           |            0.108           |            0.104           |
|                            |       Instruction For Ranking      |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |
|           ChatGPT          |                0.240               |            0.298           |            0.371           |            0.286           |            0.289           |            0.360           |            0.290           |            0.218           |            0.352           |            0.414           |            0.266           |            0.418           |            0.487           |            0.217           |            0.361           |            0.428           |            0.305           |            0.452           |            0.517           |
|            GPT-4           |                0.402               |            0.415           |            0.517           |            0.413           |            0.410           |            0.486           |            0.404           |            0.392           |            0.429           |            0.490           |            0.453           |            0.496           |            0.565           |            0.396           |            0.434           |            0.495           |            0.463           |            0.506           |            0.576           |
|                            |              Statistic             |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |                            |
|     Performance-Average    |                0.120               |            0.117           |            0.125           |            0.113           |            0.114           |            0.124           |            0.111           |            0.140           |            0.140           |            0.145           |            0.144           |            0.148           |            0.152           |            0.145           |            0.145           |            0.150           |            0.156           |            0.157           |            0.166           |
|    Performance-Variance    |                0.062               |            0.068           |            0.087           |            0.067           |            0.065           |            0.078           |            0.064           |            0.058           |            0.070           |            0.082           |            0.067           |            0.082           |            0.095           |            0.067           |            0.080           |            0.090           |            0.078           |            0.092           |            0.104           |


## Papers

- [<img src=https://img.shields.io/badge/CoRR-2023-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://arxiv.org/abs/2307.03109) [**A Survey on Evaluation of Large Language Models**](https://arxiv.org/abs/2307.03109),<br> by *Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi et al.*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2023-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://www.microsoft.com/en-us/research/publication/gpteval-nlg-evaluation-using-gpt-4-with-better-human-alignment/) [**G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment**](https://www.microsoft.com/en-us/research/publication/gpteval-nlg-evaluation-using-gpt-4-with-better-human-alignment/),<br> by *Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2023-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://doi.org/10.48550/arXiv.2302.04023) [**A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning,
Hallucination, and Interactivity**](https://doi.org/10.48550/arXiv.2302.04023),<br> by *Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji et al.*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2023-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://arxiv.org/abs/2302.06476) [**Is ChatGPT a General-Purpose Natural Language Processing Task Solver?**](https://arxiv.org/abs/2302.06476),<br> by *Qin, Chengwei, Zhang, Aston, Zhang, Zhuosheng, Chen, Jiaao, Yasunaga, Michihiro and Yang, Diyi*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2023-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://doi.org/10.48550/arXiv.2302.06466) [**ChatGPT versus Traditional Question Answering for Knowledge Graphs:
Current Status and Future Directions Towards Knowledge Graph Chatbots**](https://doi.org/10.48550/arXiv.2302.06466),<br> by *Reham Omar, Omij Mangukiya, Panos Kalnis and Essam Mansour*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2023-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://doi.org/10.48550/arXiv.2301.13867) [**Mathematical Capabilities of ChatGPT**](https://doi.org/10.48550/arXiv.2301.13867),<br> by *Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier and Julius Berner*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2023-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://doi.org/10.48550/arXiv.2302.08081) [**Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization**](https://doi.org/10.48550/arXiv.2302.08081),<br> by *Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen and Wei Cheng*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2023-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://doi.org/10.48550/arXiv.2302.12095) [**On the Robustness of ChatGPT: An Adversarial and Out-of-distribution
Perspective**](https://doi.org/10.48550/arXiv.2302.12095),<br> by *Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang et al.*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2023-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://doi.org/10.48550/arXiv.2301.04655) [**ChatGPT is not all you need. A State of the Art Review of large
Generative AI models**](https://doi.org/10.48550/arXiv.2301.04655),<br> by *Roberto Gozalo-Brizuela and Eduardo C. Garrido-Merch\'an*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2023-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://arxiv.org/abs/2302.10198) [**Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned
BERT**](https://arxiv.org/abs/2302.10198),<br> by *Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du and Dacheng Tao*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2023-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://doi.org/10.48550/arXiv.2303.07992) [**Evaluation of ChatGPT as a Question Answering System for Answering
Complex Questions**](https://doi.org/10.48550/arXiv.2303.07992),<br> by *Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen and Guilin Qi*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2023-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://arxiv.org/abs/2303.16421) [**ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models**](https://arxiv.org/abs/2303.16421),<br> by *Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu and Ben He*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2022-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://doi.org/10.48550/arXiv.2211.09110) [**Holistic Evaluation of Language Models**](https://doi.org/10.48550/arXiv.2211.09110),<br> by *Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan et al.*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2022-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://doi.org/10.48550/arXiv.2204.00498) [**Evaluating the Text-to-SQL Capabilities of Large Language Models**](https://doi.org/10.48550/arXiv.2204.00498),<br> by *Nitarshan Rajkumar, Raymond Li and Dzmitry Bahdanau*
<br><br>
- [<img src=https://img.shields.io/badge/COLING-2022-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://aclanthology.org/2022.coling-1.491) [**Are Visual-Linguistic Models Commonsense Knowledge Bases?**](https://aclanthology.org/2022.coling-1.491),<br> by *Hsiu-Yu Yang and Carina Silberer*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2022-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://doi.org/10.48550/arXiv.2212.10529) [**Is GPT-3 a Psychopath? Evaluating Large Language Models from a Psychological
Perspective**](https://doi.org/10.48550/arXiv.2212.10529),<br> by *Xingxuan Li, Yutong Li, Linlin Liu, Lidong Bing and Shafiq R. Joty*
<br><br>
- [<img src=https://img.shields.io/badge/EMNLP-2022-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://aclanthology.org/2022.emnlp-main.132) [**GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained
Language Models**](https://aclanthology.org/2022.emnlp-main.132),<br> by *Da Yin, Hritik Bansal, Masoud Monajatipoor, Liunian Harold Li and Kai-Wei Chang*
<br><br>
- [<img src=https://img.shields.io/badge/EMNLP-2022-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://aclanthology.org/2022.emnlp-main.653) [**RobustLR: A Diagnostic Benchmark for Evaluating Logical Robustness
of Deductive Reasoners**](https://aclanthology.org/2022.emnlp-main.653),<br> by *Soumya Sanyal, Zeyi Liao and Xiang Ren*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2022-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://arxiv.org/abs/2202.13169) [**A Systematic Evaluation of Large Language Models of Code**](https://arxiv.org/abs/2202.13169),<br> by *Frank F. Xu, Uri Alon, Graham Neubig and Vincent J. Hellendoorn*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2021-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://arxiv.org/abs/2107.03374) [**Evaluating Large Language Models Trained on Code**](https://arxiv.org/abs/2107.03374),<br> by *Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond\'e de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda et al.*
<br><br>
- [<img src=https://img.shields.io/badge/ACL-2021-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://doi.org/10.18653/v1/2021.findings-acl.36) [**GLGE: A New General Language Generation Evaluation Benchmark**](https://doi.org/10.18653/v1/2021.findings-acl.36),<br> by *Dayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi, Hang Zhang, Jian Jiao, Weizhu Chen, Jie Fu et al.*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2021-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://arxiv.org/abs/2104.05861) [**Evaluating Pre-Trained Models for User Feedback Analysis in Software
Engineering: A Study on Classification of App-Reviews**](https://arxiv.org/abs/2104.05861),<br> by *Mohammad Abdul Hadi and Fatemeh H. Fard*
<br><br>
- [<img src=https://img.shields.io/badge/ACL_Findings-2021-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://doi.org/10.18653/v1/2021.findings-acl.322) [**Do Language Models Perform Generalizable Commonsense Inference?**](https://doi.org/10.18653/v1/2021.findings-acl.322), [<img src=https://img.shields.io/badge/Code-skyblue alt="img" style="zoom:100%; vertical-align: middle" />](https://github.com/wangpf3/LM-for-CommonsenseInference)<br> by *Peifeng Wang, Filip Ilievski, Muhao Chen and Xiang Ren*
<br><br>
- [<img src=https://img.shields.io/badge/EMNLP-2021-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://doi.org/10.18653/v1/2021.emnlp-main.598) [**RICA: Evaluating Robust Inference Capabilities Based on Commonsense
Axioms**](https://doi.org/10.18653/v1/2021.emnlp-main.598),<br> by *Pei Zhou, Rahul Khanna, Seyeon Lee, Bill Yuchen Lin, Daniel Ho, Jay Pujara and Xiang Ren*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2020-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://arxiv.org/abs/2006.14799) [**Evaluation of Text Generation: A Survey**](https://arxiv.org/abs/2006.14799),<br> by *Asli Celikyilmaz, Elizabeth Clark and Jianfeng Gao*
<br><br>
- [<img src=https://img.shields.io/badge/CoRR-2020-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://arxiv.org/abs/2007.15780) [**Neural Language Generation: Formulation, Methods, and Evaluation**](https://arxiv.org/abs/2007.15780),<br> by *Cristina Garbacea and Qiaozhu Mei*
<br><br>
- [<img src=https://img.shields.io/badge/ICLR-2020-blue alt="img" style="zoom:100%; vertical-align: middle" />](https://openreview.net/forum?id=SkeHuCVFDr) [**BERTScore: Evaluating Text Generation with BERT**](https://openreview.net/forum?id=SkeHuCVFDr),<br> by *Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger and Yoav Artzi*
<br><br>


## LLM-List

### Pre-trained-LLM

|       Model       | Size |  Architecture  |                                                                                               Access                                                                                               |  Date  | Origin                                                                                                                        |
| :----------------: | :--: | :-------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----: | ----------------------------------------------------------------------------------------------------------------------------- |
| Switch Transformer | 1.6T |  Decoder(MOE)  |                                                                                                   -                                                                                                   | 2021-01 | [Paper](https://arxiv.org/pdf/2101.03961.pdf)                                                                                    |
|        GLaM        | 1.2T |  Decoder(MOE)  |                                                                                                   -                                                                                                   | 2021-12 | [Paper](https://arxiv.org/pdf/2112.06905.pdf)                                                                                    |
|        PaLM        | 540B |     Decoder     |                                                                                                   -                                                                                                   | 2022-04 | [Paper](https://arxiv.org/pdf/2204.02311.pdf)                                                                                    |
|       MT-NLG       | 530B |     Decoder     |                                                                                                   -                                                                                                   | 2022-01 | [Paper](https://arxiv.org/pdf/2201.11990.pdf)                                                                                    |
|      J1-Jumbo      | 178B |     Decoder     |                                                                              [api](https://docs.ai21.com/docs/complete-api)                                                                              | 2021-08 | [Paper](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)               |
|        OPT        | 175B |     Decoder     |                                                  [api](https://opt.alpa.ai) \| [ckpt](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT)                                                  | 2022-05 | [Paper](https://arxiv.org/pdf/2205.01068.pdf)                                                                                    |
|       BLOOM       | 176B |     Decoder     |                                                      [api](https://huggingface.co/bigscience/bloom) \| [ckpt](https://huggingface.co/bigscience/bloom)                                                      | 2022-11 | [Paper](https://arxiv.org/pdf/2211.05100.pdf)                                                                                    |
|      GPT 3.0      | 175B |     Decoder     |                                                                                      [api](https://openai.com/api/)                                                                                      | 2020-05 | [Paper](https://arxiv.org/pdf/2005.14165.pdf)                                                                                    |
|       LaMDA       | 137B |     Decoder     |                                                                                                   -                                                                                                   | 2022-01 | [Paper](https://arxiv.org/pdf/2201.08239.pdf)                                                                                    |
|        GLM        | 130B |     Decoder     |                                                                                [ckpt](https://github.com/THUDM/GLM-130B)                                                                                | 2022-10 | [Paper](https://arxiv.org/pdf/2210.02414.pdf)                                                                                    |
|        YaLM        | 100B |     Decoder     |                                                                               [ckpt](https://github.com/yandex/YaLM-100B)                                                                               | 2022-06 | [Blog](https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6) |
|       LLaMA       |  65B  |      Decoder      |                                                                          [ckpt](https://github.com/facebookresearch/llama)                                                                          | 2022-09 | [Paper](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)                                                                                     |
|      GPT-NeoX      | 20B |     Decoder     |                                                                              [ckpt](https://github.com/EleutherAI/gpt-neox)                                                                              | 2022-04 | [Paper](https://arxiv.org/pdf/2204.06745.pdf)                                                                                    |
|        UL2        | 20B |    agnostic    | [ckpt](https://huggingface.co/google/ul2#:~:text=UL2%20is%20a%20unified%20framework%20for%20pretraining%20models,downstream%20fine-tuning%20is%20associated%20with%20specific%20pre-training%20schemes.) | 2022-05 | [Paper](https://arxiv.org/pdf/2205.05131v1.pdf)                                                                                  |
|    鹏程.盘古α    | 13B |     Decoder     |                                                      [ckpt](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/PanGu-α#模型下载)                                                      | 2021-04 | [Paper](https://arxiv.org/pdf/2104.12369.pdf)                                                                                    |
|         T5         | 11B | Encoder-Decoder |                                                                                  [ckpt](https://huggingface.co/t5-11b)                                                                                  | 2019-10 | [Paper](https://jmlr.org/papers/v21/20-074.html)                                                                                 |
|      CPM-Bee      | 10B |     Decoder     |                                                                                [api](https://live.openbmb.org/models/bee)                                                                                | 2022-10 | [Paper](https://arxiv.org/pdf/2012.00413.pdf)                                                                                    |
|       rwkv-4       |  7B  |      RWKV      |                                                                          [ckpt](https://huggingface.co/BlinkDL/rwkv-4-pile-7b)                                                                          | 2022-09 | [Github](https://github.com/BlinkDL/RWKV-LM)                                                                                     |
|       GPT-J       |  6B  |     Decoder     |                                                                            [ckpt](https://huggingface.co/EleutherAI/gpt-j-6B)                                                                            | 2022-09 | [Github](https://github.com/kingoflolz/mesh-transformer-jax)                                                                     |
|      GPT-Neo      | 2.7B |     Decoder     |                                                                              [ckpt](https://github.com/EleutherAI/gpt-neo)                                                                              | 2021-03 | [Github](https://github.com/EleutherAI/gpt-neo)                                                                                  |
|      GPT-Neo      | 1.3B |     Decoder     |                                                                              [ckpt](https://github.com/EleutherAI/gpt-neo)                                                                              | 2021-03 | [Github](https://github.com/EleutherAI/gpt-neo)                                                                                  |

### Instruction-finetuned-LLM
|       Model       | Size |  Architecture  |                                                                                               Access                                                                                               |  Date  | Origin                                                                                                                        |
| :----------------: | :--: | :-------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----: | ----------------------------------------------------------------------------------------------------------------------------- |
|Flan-PaLM| 540B | Decoder |-|2022-10|[Paper](https://arxiv.org/pdf/2210.11416.pdf)|
|BLOOMZ| 176B | Decoder | [ckpt](https://huggingface.co/bigscience/bloomz) |2022-11|[Paper](https://arxiv.org/pdf/2211.01786.pdf)|
| InstructGPT |175B| Decoder | [api](https://platform.openai.com/overview) | 2022-03 | [Paper](https://arxiv.org/pdf/2203.02155.pdf) |
|Galactica|120B|Decoder|[ckpt](https://huggingface.co/facebook/galactica-120b)|2022-11| [Paper](https://arxiv.org/pdf/2211.09085.pdf)|
| OpenChatKit| 20B | - |[ckpt](https://github.com/togethercomputer/OpenChatKit)| 2023-3 |-|
| Flan-UL2| 20B  | Decoder | [ckpt](https://github.com/google-research/google-research/tree/master/ul2)|2023-03 | [Blog](https://www.yitay.net/blog/flan-ul2-20b)|
| Gopher | - | - | - | - | - |
| Chinchilla | - | - | - | - |- |
|Flan-T5| 11B | Encoder-Decoder |[ckpt](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)|2022-10|[Paper](https://arxiv.org/pdf/2210.11416.pdf)|
|T0|11B|Encoder-Decoder|[ckpt](https://huggingface.co/bigscience/T0)|2021-10|[Paper](https://arxiv.org/pdf/2110.08207.pdf)|
|Alpaca| 7B|Decoder|[demo](https://crfm.stanford.edu/alpaca/)|2023-03|[Github](https://github.com/tatsu-lab/stanford_alpaca)|


### Aligned-LLM
|       Model       | Size |  Architecture  |                                                                                               Access                                                                                               |  Date  | Origin                                                                                                                        |
| :----------------: | :--: | :-------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----: | ----------------------------------------------------------------------------------------------------------------------------- |
| GPT 4  | - | - | - | 2023-03 | [Blog](https://openai.com/research/gpt-4)|
|      ChatGPT      |  -  |     Decoder     |                                                                                 [demo](https://openai.com/blog/chatgpt/)\|[api](https://share.hsforms.com/1u4goaXwDRKC9-x9IvKno0A4sk30)   | 2022-11 | [Blog](https://openai.com/blog/chatgpt/)      |
| Sparrow  | 70B | - | - | 2022-09 | [Paper](https://arxiv.org/pdf/2209.14375.pdf)|
| Claude  | - | - | [demo](https://poe.com/claude)\|[api](https://www.anthropic.com/earlyaccess) | 2023-03 | [Blog](https://www.anthropic.com/index/introducing-claude) |


### Open-LLM

- [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) - A foundational, 65-billion-parameter large language model. [LLaMA.cpp](https://github.com/ggerganov/llama.cpp) [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama)
  - [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) - A model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. [Alpaca.cpp](https://github.com/antimatter15/alpaca.cpp) [Alpaca-LoRA](https://github.com/tloen/alpaca-lora)
  - [Flan-Alpaca](https://github.com/declare-lab/flan-alpaca) - Instruction Tuning from Humans and Machines.
  - [Baize](https://github.com/project-baize/baize-chatbot) - Baize is an open-source chat model trained with [LoRA](https://github.com/microsoft/LoRA). It uses 100k dialogs generated by letting ChatGPT chat with itself. 
  - [Cabrita](https://github.com/22-hours/cabrita) - A portuguese finetuned instruction LLaMA.
  - [Vicuna](https://github.com/lm-sys/FastChat) - An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality. 
  - [Llama-X](https://github.com/AetherCortex/Llama-X) - Open Academic Research on Improving LLaMA to SOTA LLM.
  - [Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna) - A Chinese Instruction-following LLaMA-based Model.
  - [GPTQ-for-LLaMA](https://github.com/qwopqwop200/GPTQ-for-LLaMa) - 4 bits quantization of [LLaMA](https://arxiv.org/abs/2302.13971) using [GPTQ](https://arxiv.org/abs/2210.17323).
  - [GPT4All](https://github.com/nomic-ai/gpt4all) - Demo, data, and code to train open-source assistant-style large language model based on GPT-J and LLaMa.
  - [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/) - A Dialogue Model for Academic Research
  - [BELLE](https://github.com/LianjiaTech/BELLE) - Be Everyone's Large Language model Engine
  - [StackLLaMA](https://huggingface.co/blog/stackllama) - A hands-on guide to train LLaMA with RLHF.
  - [RedPajama](https://github.com/togethercomputer/RedPajama-Data) -  An Open Source Recipe to Reproduce LLaMA training dataset.
  - [Chimera](https://github.com/FreedomIntelligence/LLMZoo) - Latin Phoenix.
- [BLOOM](https://huggingface.co/bigscience/bloom) - BigScience Large Open-science Open-access Multilingual Language Model [BLOOM-LoRA](https://github.com/linhduongtuan/BLOOM-LORA)
  - [BLOOMZ&mT0](https://huggingface.co/bigscience/bloomz) - a family of models capable of following human instructions in dozens of languages zero-shot.
  - [Phoenix](https://github.com/FreedomIntelligence/LLMZoo)
  
- [T5](https://arxiv.org/abs/1910.10683) - Text-to-Text Transfer Transformer 
  - [T0](https://arxiv.org/abs/2110.08207) - Multitask Prompted Training Enables Zero-Shot Task Generalization

- [OPT](https://arxiv.org/abs/2205.01068) - Open Pre-trained Transformer Language Models.
- [UL2](https://arxiv.org/abs/2205.05131v1) - a unified framework for pretraining models that are universally effective across datasets and setups. 
- [GLM](https://github.com/THUDM/GLM)- GLM is a General Language Model pretrained with an autoregressive blank-filling objective and can be finetuned on various natural language understanding and generation tasks.
  - [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) - ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 [General Language Model (GLM)](https://github.com/THUDM/GLM) 架构，具有 62 亿参数.
  - [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) -开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM2-6B 引入了更长的上下文、更好的性能和更高效的推理.
- [RWKV](https://github.com/BlinkDL/RWKV-LM) - Parallelizable RNN with Transformer-level LLM Performance.
  - [ChatRWKV](https://github.com/BlinkDL/ChatRWKV) - ChatRWKV is like ChatGPT but powered by my RWKV (100% RNN) language model.
- [StableLM](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models) - Stability AI Language Models.
- [YaLM](https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6) - a GPT-like neural network for generating and processing text. It can be used freely by developers and researchers from all over the world.
- [GPT-Neo](https://github.com/EleutherAI/gpt-neo) - An implementation of model & data parallel [GPT3](https://arxiv.org/abs/2005.14165)-like models using the [mesh-tensorflow](https://github.com/tensorflow/mesh) library.
- [GPT-J](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b) - A 6 billion parameter, autoregressive text generation model trained on [The Pile](https://pile.eleuther.ai/).
  - [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) - a cheap-to-build LLM that exhibits a surprising degree of the instruction following capabilities exhibited by ChatGPT.

- [Pythia](https://github.com/EleutherAI/pythia) - Interpreting Autoregressive Transformers Across Time and Scale
  - [Dolly 2.0](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) - the first open source, instruction-following LLM, fine-tuned on a human-generated instruction dataset licensed for research and commercial use.
- [OpenFlamingo](https://github.com/mlfoundations/open_flamingo) - an open-source reproduction of DeepMind's Flamingo model.
- [Cerebras-GPT](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/) - A Family of Open, Compute-efficient, Large Language Models.
- [GALACTICA](https://github.com/paperswithcode/galai/blob/main/docs/model_card.md) - The GALACTICA models are trained on a large-scale scientific corpus.
  - [GALPACA](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b) - GALACTICA 30B fine-tuned on the Alpaca dataset.

- [Palmyra](https://huggingface.co/Writer/palmyra-base) - Palmyra Base was primarily pre-trained with English text.
- [Camel](https://huggingface.co/Writer/camel-5b-hf) - a state-of-the-art instruction-following large language model designed to deliver exceptional performance and versatility.
- [h2oGPT](https://github.com/h2oai/h2ogpt)
- [PanGu-α](https://openi.org.cn/pangu/) - PanGu-α is a 200B parameter autoregressive pretrained Chinese language model develped by Huawei Noah's Ark Lab, MindSpore Team and Peng Cheng Laboratory.
- [MOSS](https://github.com/OpenLMLab/MOSS) - MOSS是一个支持中英双语和多种插件的开源对话语言模型.
- [Open-Assistant](https://github.com/LAION-AI/Open-Assistant) - a project meant to give everyone access to a great chat based large language model.
  - [HuggingChat](https://huggingface.co/chat/) - Powered by Open Assistant's latest model – the best open source chat model right now and @huggingface Inference API.
  - [Baichuan](https://github.com/baichuan-inc/Baichuan-13B) - An open-source, commercially available large-scale language model developed by Baichuan Intelligent Technology following Baichuan-7B, containing 13 billion parameters. (20230715)
  - - [Qwen](https://github.com/QwenLM/Qwen-7B) - Qwen-7B is the 7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-7B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. (20230803)

### Popular-LLM

|                                                                                                                                       **Model**                                                                                                                                       | **\#Author** | **\#Link** | **\#Parameter** |   **Base Model**  | **\#Layer** | **\#Encoder** | **\#Decoder** | **\#Pretrain Tokens** | **\#IFT Sample** | **RLHF** |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------:|:----------------------:|:----------------------:|:----------------------:|:----------------:|:------------------:|:------------------:|:--------------------------:|:---------------------:|:-------------:|
|                                                                                                 GPT3-Ada | brown2020language | https://platform.openai.com/docs/models/gpt-3                                                                                                 |         0.35B        |            -           |        24        |          -         |         24         |              -             |           -           |       -       |
|                                                                                                 Pythia-1B | biderman2023pythia | https://huggingface.co/EleutherAI/pythia-1b                                                                                                 |          1B          |            -           |        16        |          -         |         16         |         300B tokens        |           -           |       -       |
|                                                                                               GPT3-Babbage | brown2020language | https://platform.openai.com/docs/models/gpt-3                                                                                               |         1.3B         |            -           |        24        |          -         |         24         |              -             |           -           |       -       |
|                                                                                                        GPT2-XL | radford2019language | https://huggingface.co/gpt2-xl                                                                                                        |         1.5B         |            -           |        48        |          -         |         48         |         40B tokens         |           -           |       -       |
|                                                                                                    BLOOM-1b7 | scao2022bloom | https://huggingface.co/bigscience/bloom-1b7                                                                                                   |         1.7B         |            -           |        24        |          -         |         24         |         350B tokens        |           -           |       -       |
|                                                                                            BLOOMZ-1b7 | muennighoff2022crosslingual | https://huggingface.co/bigscience/bloomz-1b7                                                                                           |         1.7B         |        BLOOM-1b7       |        24        |          -         |         24         |              -             |      8.39B tokens     |       -       |
|                                                                                                    Dolly-v2-3b | 2023dolly | https://huggingface.co/databricks/dolly-v2-3b                                                                                                   |         2.8B         |       Pythia-2.8B      |        32        |          -         |         32         |              -             |          15K          |       -       |
|                                                                                               Pythia-2.8B | biderman2023pythia | https://huggingface.co/EleutherAI/pythia-2.8b                                                                                               |         2.8B         |            -           |        32        |          -         |         32         |         300B tokens        |           -           |       -       |
|                                                                                                     BLOOM-3b | scao2022bloom | https://huggingface.co/bigscience/bloom-3b                                                                                                    |          3B          |            -           |        30        |          -         |         30         |         350B tokens        |           -           |       -       |
|                                                                                             BLOOMZ-3b | muennighoff2022crosslingual | https://huggingface.co/bigscience/bloomz-3b                                                                                            |          3B          |        BLOOM-3b        |        30        |          -         |         30         |              -             |      8.39B tokens     |       -       |
|                                                                                       StableLM-Base-Alpha-3B | 2023StableLM | https://huggingface.co/stabilityai/stablelm-base-alpha-3b                                                                                      |          3B          |            -           |        16        |          -         |         16         |         800B tokens        |           -           |       -       |
|                                                                                      StableLM-Tuned-Alpha-3B | 2023StableLM | https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b                                                                                     |          3B          | StableLM-Base-Alpha-3B |        16        |          -         |         16         |              -             |          632K         |       -       |
|                                                                                               ChatGLM-6B | zeng2023glm-130b,du2022glm | https://huggingface.co/THUDM/chatglm-6b                                                                                              |          6B          |            -           |        28        |         28         |         28         |          1T tokens         |       \checkmark      |   \checkmark  |
|                                                                                                  DoctorGLM | xiong2023doctorglm | https://github.com/xionghonglin/DoctorGLM                                                                                                  |          6B          |       ChatGLM-6B       |        28        |         28         |         28         |              -             |         6.38M         |       -       |
|                                                                                                      ChatGLM-Med | ChatGLM-Med | https://github.com/SCIR-HI/Med-ChatGLM                                                                                                      |          6B          |       ChatGLM-6B       |        28        |         28         |         28         |              -             |           8K          |       -       |
|                                                                                                GPT3-Curie | brown2020language | https://platform.openai.com/docs/models/gpt-3                                                                                                |         6.7B         |            -           |        32        |          -         |         32         |              -             |           -           |       -       |
|                                                                                              MPT-7B-Chat | MosaicML2023Introducing | https://huggingface.co/mosaicml/mpt-7b-chat                                                                                             |         6.7B         |         MPT-7B         |        32        |          -         |         32         |              -             |          360K         |       -       |
|                                                                                          MPT-7B-Instruct | MosaicML2023Introducing | https://huggingface.co/mosaicml/mpt-7b-instruct                                                                                         |         6.7B         |         MPT-7B         |        32        |          -         |         32         |              -             |         59.3K         |       -       |
|                                                                                    MPT-7B-StoryWriter-65k+ | MosaicML2023Introducing | https://huggingface.co/mosaicml/mpt-7b-storywriter                                                                                    |         6.7B         |         MPT-7B         |        32        |          -         |         32         |              -             |       \checkmark      |       -       |
|                                                                                                    Dolly-v2-7b | 2023dolly | https://huggingface.co/databricks/dolly-v2-7b                                                                                                   |         6.9B         |       Pythia-6.9B      |        32        |          -         |         32         |              -             |          15K          |       -       |
|                                                                                       h2ogpt-oig-oasst1-512-6.9b | 2023h2ogpt | https://huggingface.co/h2oai/h2ogpt-oig-oasst1-512-6.9b                                                                                      |         6.9B         |       Pythia-6.9B      |        32        |          -         |         32         |              -             |          398K         |       -       |
|                                                                                               Pythia-6.9B | biderman2023pythia | https://huggingface.co/EleutherAI/pythia-6.9b                                                                                               |         6.9B         |            -           |        32        |          -         |         32         |         300B tokens        |           -           |       -       |
|                                                                                                     Alpaca-7B | alpaca | https://huggingface.co/tatsu-lab/alpaca-7b-wdiff                                                                                                    |          7B          |        LLaMA-7B        |        32        |          -         |         32         |              -             |          52K          |       -       |
|                                                                                                 Alpaca-LoRA-7B | 2023alpacalora | https://huggingface.co/tloen/alpaca-lora-7b                                                                                                |          7B          |        LLaMA-7B        |        32        |          -         |         32         |              -             |          52K          |       -       |
|                                                                                                  Baize-7B | xu2023baize | https://huggingface.co/project-baize/baize-lora-7B                                                                                                 |          7B          |        LLaMA-7B        |        32        |          -         |         32         |              -             |          263K         |       -       |
|                                                                                       Baize Healthcare-7B | xu2023baize | https://huggingface.co/project-baize/baize-healthcare-lora-7B                                                                                      |          7B          |        LLaMA-7B        |        32        |          -         |         32         |              -             |          201K         |       -       |
|                                                                                                 ChatDoctor | yunxiang2023chatdoctor | https://github.com/Kent0n-Li/ChatDoctor                                                                                                |          7B          |        LLaMA-7B        |        32        |          -         |         32         |              -             |          167K         |       -       |
|                                                                                                 HuaTuo | wang2023huatuo | https://github.com/scir-hi/huatuo-llama-med-chinese                                                                                                |          7B          |        LLaMA-7B        |        32        |          -         |         32         |              -             |           8K          |       -       |
|                                                                                                   Koala-7B | koala_blogpost_2023 | https://huggingface.co/young-geng/koala                                                                                                   |          7B          |        LLaMA-7B        |        32        |          -         |         32         |              -             |          472K         |       -       |
|                                                                                              LLaMA-7B | touvron2023llama | https://huggingface.co/decapoda-research/llama-7b-hf                                                                                              |          7B          |            -           |        32        |          -         |         32         |          1T tokens         |           -           |       -       |
|                                                                                               Luotuo-lora-7b-0.3 | luotuo | https://huggingface.co/silk-road/luotuo-lora-7b-0.3                                                                                              |          7B          |        LLaMA-7B        |        32        |          -         |         32         |              -             |          152K         |       -       |
|                                                                                       StableLM-Base-Alpha-7B | 2023StableLM | https://huggingface.co/stabilityai/stablelm-base-alpha-7b                                                                                      |          7B          |            -           |        16        |          -         |         16         |         800B tokens        |           -           |       -       |
|                                                                                      StableLM-Tuned-Alpha-7B | 2023StableLM | https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b                                                                                     |          7B          | StableLM-Base-Alpha-7B |        16        |          -         |         16         |              -             |          632K         |       -       |
|                                                                                            Vicuna-7b-delta-v1.1 | vicuna2023 | https://github.com/lm-sys/FastChat\#vicuna-weights                                                                                            |          7B          |        LLaMA-7B        |        32        |          -         |         32         |              -             |          70K          |       -       |
| BELLE-7B-0.2M /0.6M /1M /2M | belle2023exploring  | https://huggingface.co/BelleGroup/BELLE-7B-2M    |         7.1B         |      Bloomz-7b1-mt     |        30        |          -         |         30         |              -             |    0.2M/0.6M/1M/2M    |       -       |
|                                                                                                    BLOOM-7b1 | scao2022bloom | https://huggingface.co/bigscience/bloom-7b1                                                                                                     |         7.1B         |            -           |        30        |          -         |         30         |         350B tokens        |           -           |       -       |
|                              BLOOMZ-7b1 /mt /p3 | muennighoff2022crosslingual | https://huggingface.co/bigscience/bloomz-7b1-p3                                  |         7.1B         |        BLOOM-7b1       |        30        |          -         |         30         |              -             |      4.19B tokens     |       -       |
|                                                                                                   Dolly-v2-12b | 2023dolly | https://huggingface.co/databricks/dolly-v2-12b                                                                                                    |          12B         |       Pythia-12B       |        36        |          -         |         36         |              -             |          15K          |       -       |
|                                                                                            h2ogpt-oasst1-512-12b | 2023h2ogpt | https://huggingface.co/h2oai/h2ogpt-oasst1-512-12b                                                                                             |          12B         |       Pythia-12B       |        36        |          -         |         36         |              -             |         94.6K         |       -       |
|                                                                             Open-Assistant-SFT-4-12B | 2023openassistant | https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5                                                                               |          12B         |   Pythia-12B-deduped   |        36        |          -         |         36         |              -             |          161K         |       -       |
|                                                                                                Pythia-12B | biderman2023pythia | https://huggingface.co/EleutherAI/pythia-12b                                                                                                  |          12B         |            -           |        36        |          -         |         36         |         300B tokens        |           -           |       -       |
|                                                                                                 Baize-13B | xu2023baize | https://huggingface.co/project-baize/baize-lora-13B                                                                                                  |          13B         |        LLaMA-13B       |        40        |          -         |         40         |              -             |          263K         |       -       |
|                                                                                                   Koala-13B | koala_blogpost_2023 | https://huggingface.co/young-geng/koala                                                                                                    |          13B         |        LLaMA-13B       |        40        |          -         |         40         |              -             |          472K         |       -       |
|                                                                                             LLaMA-13B | touvron2023llama | https://huggingface.co/decapoda-research/llama-13b-hf                                                                                               |          13B         |            -           |        40        |          -         |         40         |          1T tokens         |           -           |       -       |
|                                                                                           StableVicuna-13B | 2023StableLM | https://huggingface.co/CarperAI/stable-vicuna-13b-delta                                                                                            |          13B         |      Vicuna-13B v0     |        40        |          -         |         40         |              -             |          613K         |   \checkmark  |
|                                                                                            Vicuna-13b-delta-v1.1 | vicuna2023 | https://github.com/lm-sys/FastChat\#vicuna-weights                                                                                             |          13B         |        LLaMA-13B       |        40        |          -         |         40         |              -             |          70K          |       -       |
|                                                                                                 moss-moon-003-sft | 2023moss | https://huggingface.co/fnlp/moss-moon-003-sft                                                                                                   |          16B         |   moss-moon-003-base   |        34        |          -         |         34         |              -             |          1.1M         |       -       |
|                                                                                          moss-moon-003-sft-plugin | 2023moss | https://huggingface.co/fnlp/moss-moon-003-sft-plugin                                                                                            |          16B         |   moss-moon-003-base   |        34        |          -         |         34         |              -             |          1.4M         |       -       |
|                                                                                                    GPT-NeoX-20B | gptneox | https://huggingface.co/EleutherAI/gpt-neox-20b                                                                                                     |          20B         |            -           |        44        |          -         |         44         |            825GB           |           -           |       -       |
|                                                                                            h2ogpt-oasst1-512-20b | 2023h2ogpt | https://huggingface.co/h2oai/h2ogpt-oasst1-512-20b                                                                                             |          20B         |      GPT-NeoX-20B      |        44        |          -         |         44         |              -             |         94.6K         |       -       |
|                                                                                                 Baize-30B | xu2023baize | https://huggingface.co/project-baize/baize-lora-30B                                                                                                  |          33B         |        LLaMA-30B       |        60        |          -         |         60         |              -             |          263K         |       -       |
|                                                                                             LLaMA-30B | touvron2023llama | https://huggingface.co/decapoda-research/llama-30b-hf                                                                                               |          33B         |            -           |        60        |          -         |         60         |         1.4T tokens        |           -           |       -       |
|                                                                                             LLaMA-65B | touvron2023llama | https://huggingface.co/decapoda-research/llama-65b-hf                                                                                               |          65B         |            -           |        80        |          -         |         80         |         1.4T tokens        |           -           |       -       |
|                                                                                               GPT3-Davinci | brown2020language | https://platform.openai.com/docs/models/gpt-3                                                                                                 |         175B         |            -           |        96        |          -         |         96         |         300B tokens        |           -           |       -       |
|                                                                                                        BLOOM | scao2022bloom | https://huggingface.co/bigscience/bloom                                                                                                         |         176B         |            -           |        70        |          -         |         70         |         366B tokens        |           -           |       -       |
|                                      BLOOMZ /mt /p3 | muennighoff2022crosslingual | https://huggingface.co/bigscience/bloomz-p3                                          |         176B         |          BLOOM         |        70        |          -         |         70         |              -             |      2.09B tokens     |       -       |
|                                                                                            ChatGPT~(2023.05.01) | openaichatgpt | https://platform.openai.com/docs/models/gpt-3-5                                                                                              |           -          |         GPT-3.5        |         -        |          -         |          -         |              -             |       \checkmark      |   \checkmark  |
|                                                                                              GPT-4~(2023.05.01) | openai2023gpt4 | https://platform.openai.com/docs/models/gpt-4                                                                                               |           -          |            -           |         -        |          -         |          -         |              -             |       \checkmark      |   \checkmark  |

## Frameworks-for-Training

- [Accelerate](https://github.com/huggingface/accelerate) ![](https://img.shields.io/github/stars/huggingface/accelerate.svg?style=social) - 🚀 A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision.
- [Apache MXNet](https://github.com/apache/mxnet) ![](https://img.shields.io/github/stars/apache/mxnet.svg?style=social) - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler.
- [Caffe](https://github.com/BVLC/caffe) ![](https://img.shields.io/github/stars/BVLC/caffe.svg?style=social) - A fast open framework for deep learning.
- [ColossalAI](https://github.com/hpcaitech/ColossalAI) ![](https://img.shields.io/github/stars/hpcaitech/ColossalAI.svg?style=social) - An integrated large-scale model training system with efficient parallelization techniques.
- [DeepSpeed](https://github.com/microsoft/DeepSpeed) ![](https://img.shields.io/github/stars/microsoft/DeepSpeed.svg?style=social) - DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.
- [Horovod](https://github.com/horovod/horovod) ![](https://img.shields.io/github/stars/horovod/horovod.svg?style=social) - Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.
- [Jax](https://github.com/google/jax) ![](https://img.shields.io/github/stars/google/jax.svg?style=social) - Autograd and XLA for high-performance machine learning research.
- [Kedro](https://github.com/kedro-org/kedro) ![](https://img.shields.io/github/stars/kedro-org/kedro.svg?style=social) - Kedro is an open-source Python framework for creating reproducible, maintainable and modular data science code.
- [Keras](https://github.com/keras-team/keras) ![](https://img.shields.io/github/stars/keras-team/keras.svg?style=social) - Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow.
- [LightGBM](https://github.com/microsoft/LightGBM) ![](https://img.shields.io/github/stars/microsoft/LightGBM.svg?style=social) - A fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.
- [MegEngine](https://github.com/MegEngine/MegEngine) ![](https://img.shields.io/github/stars/MegEngine/MegEngine.svg?style=social) - MegEngine is a fast, scalable and easy-to-use deep learning framework, with auto-differentiation.
- [metric-learn](https://github.com/scikit-learn-contrib/metric-learn) ![](https://img.shields.io/github/stars/scikit-learn-contrib/metric-learn.svg?style=social) - Metric Learning Algorithms in Python.
- [MindSpore](https://github.com/mindspore-ai/mindspore) ![](https://img.shields.io/github/stars/mindspore-ai/mindspore.svg?style=social) - MindSpore is a new open source deep learning training/inference framework that could be used for mobile, edge and cloud scenarios.
- [Oneflow](https://github.com/Oneflow-Inc/oneflow) ![](https://img.shields.io/github/stars/Oneflow-Inc/oneflow.svg?style=social) - OneFlow is a performance-centered and open-source deep learning framework.
- [PaddlePaddle](https://github.com/PaddlePaddle/Paddle) ![](https://img.shields.io/github/stars/PaddlePaddle/Paddle.svg?style=social) - Machine Learning Framework from Industrial Practice.
- [PyTorch](https://github.com/pytorch/pytorch) ![](https://img.shields.io/github/stars/pytorch/pytorch.svg?style=social) - Tensors and Dynamic neural networks in Python with strong GPU acceleration.
- [PyTorch Lightning](https://github.com/lightning-AI/lightning) ![](https://img.shields.io/github/stars/lightning-AI/lightning.svg?style=social) - Deep learning framework to train, deploy, and ship AI products Lightning fast.
- [XGBoost](https://github.com/dmlc/xgboost) ![](https://img.shields.io/github/stars/dmlc/xgboost.svg?style=social) - Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library.
- [scikit-learn](https://github.com/scikit-learn/scikit-learn) ![](https://img.shields.io/github/stars/scikit-learn/scikit-learn.svg?style=social) - Machine Learning in Python.
- [TensorFlow](https://github.com/tensorflow/tensorflow) ![](https://img.shields.io/github/stars/tensorflow/tensorflow.svg?style=social) - An Open Source Machine Learning Framework for Everyone.
- [VectorFlow](https://github.com/Netflix/vectorflow) ![](https://img.shields.io/github/stars/Netflix/vectorflow.svg?style=social) - A minimalist neural network library optimized for sparse data and single machine environments.


## LLMOps

| 名称                                                    | Stars 数                                                     | 介绍                                                         |
| ------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| [agenta](https://github.com/Agenta-AI/agenta)           | ![](https://img.shields.io/github/stars/Agenta-AI/agenta.svg?style=social) | 用于构建强大LLM应用的LLMOps平台。轻松尝试和评估不同提示、模型和工作流，以构建稳健的应用程序。 |
| [Arize-Phoenix](https://github.com/Arize-ai/phoenix)    | ![](https://img.shields.io/github/stars/Arize-ai/phoenix.svg?style=social) | 用于LLMs、视觉、语言和表格模型的ML可观测性。                 |
| [BudgetML](https://github.com/ebhy/budgetml)            | ![](https://img.shields.io/github/stars/ebhy/budgetml.svg?style=social) | 在不到10行代码的情况下，以有限的预算部署ML推理服务。         |
| [CometLLM](https://github.com/comet-ml/comet-llm)       | ![](https://img.shields.io/github/stars/comet-ml/comet-llm.svg?style=social) | 100%开源的LLMOps平台，用于记录、管理和可视化LLM提示和链条。跟踪提示模板、提示变量、提示持续时间、令牌使用等其他元数据。评分提示输出并在单个UI中可视化聊天历史。 |
| [deeplake](https://github.com/activeloopai/deeplake)    | ![](https://img.shields.io/github/stars/activeloopai/Hub.svg?style=social) | 流式传输大型多模态数据集，实现接近100%的GPU利用率。查询、可视化和版本控制数据。无需重新计算模型微调的嵌入即可访问数据。 |
| [Dify](https://github.com/langgenius/dify)              | ![](https://img.shields.io/github/stars/langgenius/dify.svg?style=social) | 开源框架旨在使开发人员（甚至非开发人员）能够快速构建基于大型语言模型的有用应用程序，确保它们具有可视性、可操作性和可改进性。 |
| [Dstack](https://github.com/dstackai/dstack)            | ![](https://img.shields.io/github/stars/dstackai/dstack.svg?style=social) | 在任何云（AWS、GCP、Azure、Lambda等）中进行经济高效的LLM开发。 |
| [Embedchain](https://github.com/embedchain/embedchain)  | ![](https://img.shields.io/github/stars/embedchain/embedchain.svg?style=social) | 用于在数据集上创建类似ChatGPT的机器人的框架。                |
| [GPTCache](https://github.com/zilliztech/GPTCache)      | ![](https://img.shields.io/github/stars/zilliztech/GPTCache.svg?style=social) | 创建语义缓存以存储LLM查询的响应。                            |
| [Haystack](https://github.com/deepset-ai/haystack)      | ![](https://img.shields.io/github/stars/deepset-ai/haystack.svg?style=social) | 快速构建带有LLM代理、语义搜索、问答等功能的应用程序。        |
| [langchain](https://github.com/hwchase17/langchain)     | ![](https://img.shields.io/github/stars/hwchase17/langchain.svg?style=social) | 通过可组合性构建LLM应用程序。                                |
| [LangFlow](https://github.com/logspace-ai/langflow)     | ![](https://img.shields.io/github/stars/logspace-ai/langflow.svg?style=social) | 通过拖放组件和聊天界面轻松实验和原型化LangChain流程的无忧方式。 |
| [LangKit](https://github.com/whylabs/langkit)           | ![](https://img.shields.io/github/stars/whylabs/langkit.svg?style=social) | 开箱即用的LLM遥测收集库，可提取有关LLM随时间性能如何的特征和提示、响应和元数据的配置文件，以规模找到问题。 |
| [LiteLLM 🚅](https://github.com/BerriAI/litellm/)        | ![](https://img.shields.io/github/stars/BerriAI/litellm.svg?style=social) | 一个简单且轻量的100行包，用于跨OpenAI、Azure、Cohere、Anthropic、Replicate API端点标准化LLM API调用。 |
| [LlamaIndex](https://github.com/jerryjliu/llama_index)  | ![](https://img.shields.io/github/stars/jerryjliu/llama_index.svg?style=social) | 提供一个中央接口，将您的LLMs与外部数据连接起来。             |
| [LLMApp](https://github.com/pathwaycom/llm-app)         | ![](https://img.shields.io/github/stars/pathwaycom/llm-app.svg?style=social) | LLM App是一个Python库，帮助您以几行代码构建实时的LLM启用数据管道。 |
| [LLMFlows](https://github.com/stoyan-stoyanov/llmflows) | ![](https://img.shields.io/github/stars/stoyan-stoyanov/llmflows.svg?style=social) | LLMFlows是一个用于构建简单、明确和透明的LLM应用程序的框架，例如聊天机器人、问答系统和代理。 |
| [LLMonitor](https://github.com/llmonitor/llmonitor) | ![](https://img.shields.io/github/stars/llmonitor/llmonitor.svg?style=social) | 用于AI应用程序和代理的可观测性和监控。通过强大的跟踪和日志记录来调试代理。使用分析工具深入研究请求历史。开发人员友好的模块，可以轻松集成到LangChain中。 |
| [magentic](https://github.com/jackmpcollins/magentic) | ![](https://img.shields.io/github/stars/jackmpcollins/magentic.svg?style=social) | 无缝集成LLMs作为Python函数。使用类型注释来指定结构化输出。将LLM查询和函数调用与常规Python代码混合使用，创建复杂的LLM驱动功能。        |
| [Pezzo 🕹️](https://github.com/pezzolabs/pezzo)     | ![](https://img.shields.io/github/stars/pezzolabs/pezzo.svg?style=social)     | Pezzo是为开发人员和团队构建的开源LLMOps平台。仅需两行代码，您就可以轻松排除AI操作中的问题，协作和管理您的提示，在一个地方立即部署更改。       |
| [promptfoo](https://github.com/typpo/promptfoo)     | ![](https://img.shields.io/github/stars/typpo/promptfoo.svg?style=social)     | 用于测试和评估提示质量的开源工具。创建测试用例，自动检查输出质量并捕获回归，降低评估成本。                            |
| [prompttools](https://github.com/hegelai/prompttools) | ![](https://img.shields.io/github/stars/hegelai/prompttools.svg?style=social) | 用于测试和尝试提示的开源工具。核心思想是使开发人员能够使用熟悉的界面（如代码和笔记本）评估提示。只需几行代码，您就可以在不同模型之间测试提示和参数（无论您是使用OpenAI、Anthropic还是LLaMA模型）。您甚至可以评估向量数据库的检索准确性。 |
| [TrueFoundry](https://www.truefoundry.com/)     | 无Github链接 | 在自己的Kubernetes（EKS、AKS、GKE、On-prem）基础设施上部署LLMOps工具，包括Vector DBs、嵌入式服务器等，包括部署、微调、跟踪提示和提供完整的数据安全性和最佳GPU管理的开源LLM模型。使用最佳的软件工程实践在生产规模上训练和启动您的LLM应用程序。 |
| [ReliableGPT 💪](https://github.com/BerriAI/reliableGPT/) | ![](https://img.shields.io/github/stars/BerriAI/reliableGPT.svg?style=social) | 为您的生产LLM应用程序处理OpenAI错误（超载的OpenAI服务器、旋转的密钥或上下文窗口错误）。                                       |
| [Weights & Biases (Prompts)](https://docs.wandb.ai/guides/prompts) | 无Github链接 | 开发者优先的W&B MLOps平台中的一套LLMOps工具。利用W&B Prompts来可视化和检查LLM执行流程，跟踪输入和输出，查看中间结果，安全管理提示和LLM链配置。 |
| [xTuring](https://github.com/stochasticai/xturing)   | ![](https://img.shields.io/github/stars/stochasticai/xturing.svg?style=social)   | 使用快速和高效的微调来构建和控制您的个人LLMs。                                                                     |
| [ZenML](https://github.com/zenml-io/zenml)         | ![](https://img.shields.io/github/stars/zenml-io/zenml.svg?style=social)         | 用于编排、实验和部署生产级ML解决方案的开源框架，具有内置的`langchain`和`llama_index`集成。    |

## Courses

- [大语言模型课程notebooks集-Large Language Model Course](https://github.com/mlabonne/llm-course) - Course with a roadmap and notebooks to get into Large Language Models (LLMs).
- [Full+Stack+LLM+Bootcamp](https://ihower.tw/notes/技術筆記-AI/Full+Stack+LLM+Bootcamp) - LLM相关学习/应用资源集.

## Others

- [Evaluating Language Models by OpenAI, DeepMind, Google, Microsoft](https://levelup.gitconnected.com/how-to-benchmark-language-models-by-openai-deepmind-google-microsoft-783d4307ec50) - Evaluating Language Models by OpenAI, DeepMind, Google, Microsoft.
- [Efficient Finetuning of Quantized LLMs --- 低资源的大语言模型量化训练/部署方案](https://github.com/jianzhnie/Efficient-Tuning-LLMs) - 旨在构建和开源遵循指令的baichuan/LLaMA/Pythia/GLM中文大模型微调训练方法，该方法可以在单个 Nvidia RTX-2080TI上进行训练，多轮聊天机器人可以在单个 Nvidia RTX-3090上进行上下文长度 2048的模型训练。使用bitsandbytes进行量化，并与Huggingface的PEFT和transformers库集成.

## Other-Awesome-Lists

- [Awesome LLM](https://github.com/Hannibal046/Awesome-LLM/) -  A curated list of papers about large language models.
- [Awesome-Efficient-LLM](https://github.com/horseee/Awesome-Efficient-LLM) - A curated list for Efficient Large Language Models.
- [Awesome-production-machine-learning](https://github.com/EthicalML/awesome-production-machine-learning) - A curated list of awesome open source libraries to deploy, monitor, version and scale your machine learning.
- [Awesome-marketing-datascience](https://github.com/underlines/awesome-marketing-datascience) - Curated list of useful LLM / Analytics / Datascience resources.
- [Awesome-llm-tools](https://github.com/underlines/awesome-marketing-datascience/blob/master/llm-tools.md) - Curated list of useful LLM tool.
- [Awesome-LLM-Compression](https://github.com/HuangOwen/Awesome-LLM-Compression) - A curated list for Efficient LLM Compression.
- [Awesome-LLMOps](https://github.com/tensorchord/Awesome-LLMOps) - An awesome & curated list of the best LLMOps tools for developers.
- [Awesome-MLops](https://github.com/visenger/awesome-mlops) - An awesome list of references for MLOps - Machine Learning Operations.
- [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts) - A collection of prompt examples to be used with the ChatGPT model.
- [awesome-chatgpt-prompts-zh](https://github.com/PlexPt/awesome-chatgpt-prompts-zh) - A Chinese collection of prompt examples to be used with the ChatGPT model.
- [Awesome ChatGPT](https://github.com/humanloop/awesome-chatgpt) - Curated list of resources for ChatGPT and GPT-3 from OpenAI.
- [Chain-of-Thoughts Papers](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers) -  A trend starts from "Chain of Thought Prompting Elicits Reasoning in Large Language Models.
- [Instruction-Tuning-Papers](https://github.com/SinclairCoder/Instruction-Tuning-Papers) - A trend starts from `Natrural-Instruction` (ACL 2022), `FLAN` (ICLR 2022) and `T0` (ICLR 2022).
- [LLM Reading List](https://github.com/crazyofapple/Reading_groups/) - A paper & resource list of large language models.
- [Reasoning using Language Models](https://github.com/atfortes/LM-Reasoning-Papers) - Collection of papers and resources on Reasoning using Language Models.
- [Chain-of-Thought Hub](https://github.com/FranxYao/chain-of-thought-hub) - Measuring LLMs' Reasoning Performance
- [Awesome GPT](https://github.com/formulahendry/awesome-gpt) - A curated list of awesome projects and resources related to GPT, ChatGPT, OpenAI, LLM, and more.
- [Awesome GPT-3](https://github.com/elyase/awesome-gpt3) - a collection of demos and articles about the [OpenAI GPT-3 API](https://openai.com/blog/openai-api/).

## Licenses

[![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)

本项目遵循 [MIT License](https://lbesson.mit-license.org/).

[![CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](http://creativecommons.org/licenses/by-nc-sa/4.0/)

本项目遵循 [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).



## 引用

如果我们的项目对您有帮助，请引用我们的项目。

```
@misc{junwang2023,
  author = {Jun Wang},
  title = {Awesome-LLM-Eval: a curated list of tools, benchmarks, demos, papers for Large Language Models Evaluation},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/onejune2018/Awesome-LLM-Eval}},
}
```

作者简介： 
- SFE平台算法研发负责
- 医药发现大模型MPG([Molecular Pretraining GraphTransformer](https://github.com/onejune2018/MPG))作者
- 国际竞赛SemEval2022习语判别任务第一、MIT AI-Cure第一、VQA2021第一、TREC2021第一、EAD2019第一
- 主页：https://onejune2018.github.io/homepage/
- Google Scholar: https://scholar.google.com/citations?user=0Be01PgAAAAJ&hl=en
